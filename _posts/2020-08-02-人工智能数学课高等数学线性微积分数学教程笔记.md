---
layout: post
title: '人工智能数学课高等数学线性微积分数学教程笔记'
date: 2017-04-18
categories: 数学
tags: math
---

# 人工智能数学课高等数学线性微积分数学教程笔记

对[人工智能数学课高等数学线性微积分数学教程](https://www.bilibili.com/video/BV1s7411r7fX?p=1)的学习笔记。主要用于快速回忆已学的数学知识点，不适合基础学习。

# 1. 数学内容概述

人工智能是应用数学，需要的数学知识包括：

- 微积分
  + 主要内容：导数与求导公式，一阶导数与函数的单调性，一元函数极值判定法则，高阶导数，二阶导数与函数的凹凸性，一元导数泰勒展开。
  + 主要用**微**分部分，**求函数极值**，即机器学习库中的求解器 (solver) 的功能；
  + 推荐书籍：高等数学（第七版，同济大学数学系），数学分析新讲(张筑生)
  + 知识点：
    - [ ] 导数与偏导数定义与计算；
    - [ ] 梯度向量；
    - [ ] 极值定理，可导函数在极值点处导数或梯度必须为0；
    - [ ] 雅克比矩阵，向量到向量映射函数的偏导数构成的矩阵，用于求导推导；
    - [ ] Hessian 矩阵，二阶导数对多元函数的推广，求函数极值；
    - [ ] 凸函数的定义与判断；
    - [ ] **泰勒展开公式**，多元函数泰勒展开公式，推导出梯度下降法，牛顿法，拟牛顿法等一系列最优化方法；
    - [ ] 拉格朗日乘数法，拉格朗日对偶，求解带等式约束的极值问题。
- 线性代数
  + 主要内容：向量及其运算，矩阵及其运算，张量，行列式，二次型，特征值与特征向量。
  + 推荐书籍：工程数学线性代数(同济大学数学系)
  + 知识点：
    - [ ] 向量及其运算，包括加法，减法，数乘，转置，内积；
    - [ ] 向量和矩阵的范数，$L1$ 范数(曼哈顿距离)和 $L2$ 范数(两点间距离)；
    - [ ] 矩阵及其运算，包括加法，减法乘法和数乘；
    - [ ] 逆矩阵定义和性质；
    - [ ] 行列式定义和计算；
    - [ ] 二次型定义；
    - [ ] 矩阵正定性；
    - [ ] 矩阵的特征值 (**eigenvalue**) 与特征向量 (**eigenvector**) ；
    - [ ] 矩阵的奇异值分解 (SVD) ；
    - [ ] 线性方程组的数值解法，尤其是共轭梯度法。
  + 数据一般都是向量、矩阵或张量。
- 概率论
  - 主要内容：随机事件与概率，条件概率和贝叶斯公式，随机变量，随机变量的期望和方差，常用概率分布(正态分布，均匀分布，伯努利二项分布)，随机向量(联合概率密度函数等)，协方差和协方差矩阵，最大似然估计。
  - 将所处理的样本数据看作随机变量/向量，用概率论的观点对问题进行建模。
  - 推荐书籍：概率论与数理统计（第四版）
  - 知识点：
    - [ ] 随机事件概念，概率定义与计算；
    - [ ] 随机变量与概率分布，尤其是连续性随机变量的概率密度函数和分布函数；
    - [ ] 条件概率与贝叶斯公式；
    - [ ] 常见概率分布，正态分布，均匀分布，伯努利二项分布；
    - [ ] 随机变量的均值方差，协方差；
    - [ ] 随机变量的独立性；
    - [ ] 最大似然估计。

- 最优化

  - 推荐书籍：凸优化(作者：Stephen Boyd, Lieven Vandenberghe著 王书宁，许鋆，黄晓霖译)，非线性规划（第2版，作者：Dimitri P. Bertsekas 著 宋士吉、张玉利、贾庆山 译）
  - 凸优化问题

- 其他

  微分几何，泛函分析和识别函数，离散数学(图论)。

# 2. 一元函数微分学

## - 导数

### - 导数的定义

${f}'\left ( x_0 \right )= \lim\limits_{\Delta x\rightarrow 0}\frac{f( x_0+\Delta x )-f(x_0)}{\Delta x}$

### - 左导数、右导数和右导数

- ReLu函数 Activation Function

$\max(0,x)$

### - 几何意义与物理意义

- 几何意义，切线的斜率(一元函数)
- 物理意义，瞬时速度

$f'(t)=\lim\limits_{\Delta t\rightarrow 0}\frac {\Delta s}{\Delta t}$

## - 求导公式

### - 基本函数

- $(x^a)'=ax^{a-1}$
- $(a^x)'=a^x\ln a$ ; $(e^x)'=e^x$
- $(log_a x)'=\frac{1}{\ln a}\frac{1}{x}$ ; $(\ln x)'=\frac{1}{x}$
- 定义：$\lim\limits_{n\rightarrow+\infty}(1+\frac{1}{n})^n=e$ ;  $\lim\limits_{x\rightarrow0}\frac{\sin x}{x}=1$; $\lim\limits_{n\rightarrow0}(1+n)^\frac{1}{n}=e$

### - 四则运算法则

- $(f(x)+g(x))'=f'(x)+g'(x)$
- $(f(x)g(x))'=f'(x)g(x)+f(x)g'(x)$
- $(\frac{f(x)}{g(x)})'=\frac{f'(x)g(x)-f(x)g'(x)}{g^2(x)}$

### - 复合函数求导法则

$(f(g(x)))'=f'(g)g'(x)$ 链式求导法则

## - 用途

- 求极值，backpropagation 激活函数

- sigmoid函数：$\sigma(x)=\frac{1}{1+e^{-x}}$ ; $\sigma'(x)=[1-\sigma(x)]\sigma(x)$

- tanh (双曲正切)函数：$\tanh(x)=\frac{\sinh x}{\cosh x}=\frac{e^x-e^{-x}}{e^x+e^{-x}}$

  $\tanh'(x)=1-\tanh^2(x)$


## - 高阶导数

对导数再次求导：$f^{(n)}(x)$

##  - 导数与函数单调性关系

导数大于0，单调增；导数小于0，单调减。

## - 极值定理

极值处函数的导数（若有）等于0，导数等于0处不一定是极值。

极值点是函数图像的某段子区间内上极大值或者极小值点的**横坐标**。极值点出现在函数的驻点（导数为0的点）或不可导点处（导函数不存在，也可以取得极值，此时驻点不存在）。

## - 导数与函数凹凸性

注：中国大陆数学界某些机构关于函数凹凸性定义和国外的定义是[相反](https://www.sohu.com/a/214387081_553401)的。

国外定义中，凸函数是**向下凸**的（不同于我们传统意义上的"凸"）。

凹凸函数定义：

> 设函数 $f$ 为定义在区间 $I$ 上的函数，若对 $(a,b)$ 上任意两点 $x_1$ 、 $x_2$ ，恒有：
>
> (1) $f(\frac{x_1+x_2}{2})>\frac{f(x_1)+f(x_2)}{2}$，则称 $f$ 为 $(a,b)$ 上的凹函数或者上凸函数或者 A 型函数；
>
> (2) $f(\frac{x_1+x_2}{2})<\frac{f(x_1)+f(x_2)}{2}$，则称 $f$ 为 $(a,b)$ 上的凸函数或者下凸函数或者 V 型函数；

- $f''(x)>0$ 二阶导数大于0——凸 (convex) 函数；反之，凹 (concave) 函数
- $f'(x)=0$ 驻点；
- $f''(x)=0$ ，若该曲线图形的函数在拐点(连续曲线的凹弧与凸弧的分界点)有二阶导数，则二阶导数在拐点处异号（由正变负或由负变正）或不存在。

## - 一元函数泰勒展开

多项式函数来近似一个可导函数。

$$
f(x)=\frac{f(x_0)}{0!}+\frac{f'(x_0)}{1!}(x-x_0)+\frac{f''(x_0)}{2!}(x-x_0)^2+...+\frac{f^{n}(x_0)}{n!}(x-x_0)^n + R_n(x)
$$

$$
f(x)=f(x_k)+(x-x_k)f'(x_k)+\frac{1}{2}(x-x_k)^2f''(x_k)+o^n
$$

机器学习中求极值用的，梯度下降法（保留泰勒展开一阶项），牛顿法（保留泰勒展开二阶项）。

# 3. 线性代数基础

## - 向量

- 一维数组，几何意义是空间中的点，$n$ 维向量集合的全体构成了 $n$ 维欧式空间 $R^n$ ;

- 行向量(编程语言中把数据存为它)，列向量(数学上常用)；

- 向量运算

  - 加减：分量加减， $Error=\boldsymbol{y}-\hat{\boldsymbol{y}}$

  - 数乘：数和分量相乘，$\boldsymbol{w}^{t+1}=\boldsymbol{w}^t-\alpha \boldsymbol{g}$

  - 向量的内积(点乘)：$X^TY$

    ```python
    np.dot(a,b)
    ```

  - 运算法则：$A+B+C=A+(B+C)$ ; $k*(X+Y)=kX+kY$

## - 向量的范数

- 范数：$$\left \| \boldsymbol{x} \right \|_p=\left ( \sum\limits_{i=1}^{n}\left | x_i \right |^p \right )^\frac{1}{p}$$; $p$ 为整数，向量变成标量；
- 1 范数是绝对值加和，记为 $L1$: $$\left \| \boldsymbol{x} \right \|_1=\sum\limits_{i=1}^n \left | x_i \right |$$，曼哈顿距离；
- 2 范数是向量长度，向量的模，记为 $L2$: $$\left \| \boldsymbol{x} \right \|_2=\sqrt{\sum\limits_{i=1}^n \left ( x_i \right )^2}$$，欧式距离；
- 应用：$L1$ 正则项：$$\sum\limits_{i=1}^n \left | w_i \right |$$；$L2$ 正则项：$$\sum\limits_{i=1}^n \left |w_i \right |^2$$；正则项越小，模型容错性越强，防止过拟合；

## - 特殊向量

- 0 向量：`np.zeros()`; 全 1 向量：`np.ones()`;
- 稀疏向量(Sparse vector)；稠密向量(Dense vector)；one-hot 编码；
- 单位向量，长度为1；

## - 矩阵

- 二维数组，方阵，对称矩阵：$a_{ij}=a_{ji}$
- 单位阵：`np.identity()`、`np.eye()`
- 矩阵运算：加减、数乘、转置(`a.T`、`a.transpose(1,0)`)

- 矩阵的乘法：`a*b` `a/b`(按对应位相乘除)，`np.dot(a,b)` 是把第一个矩阵的每一行和第二个矩阵的每一列做内积。
- $A+B+C=A+(B+C)$; $(AB)C=A(BC)$; $(A+B)C=AC+BC$; $A(B+C)=AB+AC$; $AB\ne BA$
- 转置公式：$(AB)^T=B^TA^T$

## - 逆矩阵

- 假设一个矩阵 $A$ (方阵)，有 $AB=BA=I$ ， $B=A^{-1}$
- $(AB)^{-1}=B^{-1}A^{-1}$，$(A^{-1})^{-1}=A$，$(A^T)^{-1}=(A^{-1})T$
- `np.linalg.inv(A)` (linear algebra)

## - 行列式

- 行列式把矩阵(方阵)变成一个标量。

$$
\begin{vmatrix}a_{11} & a_{12} \\a_{21} & a_{22}\end{vmatrix}=a_{11}a_{22}-a_{12}a_{21}
$$

$$
\begin{vmatrix}a_{11}& a_{12} &a_{13}\\a_{21}& a_{22} &a_{23}\\ a_{31}& a_{32} &a_{33}\end{vmatrix}=a_{11}a_{22}a_{33}+a_{12}a_{23}a_{31}+a_{13}a_{21}a_{32}-a_{13}a_{22}a_{31}-a_{12}a_{21}a_{33}-a_{11}a_{23}a_{32}
$$

- 行列式性质：

  $$\left | AB \right |=\left | A \right |\left | B \right |$， $\left | A^{-1} \right |=\left | A \right |^{-1}$，$\left | \alpha A \right |=\alpha ^n\left | A \right |$$

- `np.linalg.det(A)`

# 4. 多元函数的微分学

## - 偏导数

其他的自变量固定不动，对其中某一个变量求导数。
$$
\frac{\partial f}{\partial x_i}=\lim \limits_{\Delta x_i\rightarrow 0}\frac{f\left ( x_1,...,x_i+\Delta x_i,...,x_n \right )-f\left ( x_1,...,x_i,...,x_n \right )}{\Delta x_i}
$$

```python
from sympy import diff,symbols
x,y = symbols('x y')
f = x**2 + x*y - y**2
diff(f,x)
>>> 2*x + y
```

## - 高阶偏导数

- 依次对每一个变量反复求导
- 高阶导数和求导次序无关：$\frac{\partial ^2f}{\partial x\partial y}=\frac{\partial ^2f}{\partial y\partial x}$; 
- `diff(f,x,2)` = $\frac{\partial ^2f}{\partial^2 x}$
- `diff(f,y).subs(y,2)` = $$\frac{\partial f}{\partial y}\Big |_{y=2}$$

## - 梯度

$$
\nabla f(\boldsymbol{x})=\left ( \frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2},\cdots,\frac{\partial f}{\partial x_n}\right )^T
$$

## - 雅克比矩阵

一阶偏导数构成的矩阵，简化求导公式。

一个函数 $f$ 把 $n$ 维向量 $\boldsymbol{x}$ 映射为 $k$ 维向量 $\boldsymbol{y}$：$\boldsymbol{y}=f(\boldsymbol{x})$
$$
\begin{bmatrix}
\frac{\partial y_1}{\partial x_1} & \frac{\partial y_1}{\partial x_2} & \cdots & \frac{\partial y_1}{\partial x_n}\\ 
 \frac{\partial y_2}{\partial x_1} & \frac{\partial y_2}{\partial x_2} & \cdots & \frac{\partial y_2}{\partial x_n} \\ 
\cdots & \cdots & \cdots & \cdots\\ 
 \frac{\partial y_k}{\partial x_1} & \frac{\partial y_k}{\partial x_2} & \cdots & \frac{\partial y_k}{\partial x_n}
\end{bmatrix}
$$
第 $k$ 行就是 $y_k$ 对 $x_1,x_2,\cdots,x_n$ 求偏导。

## - Hessian 矩阵

- 设有一个 $n$ 元函数：

$$
\begin{bmatrix}
\frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1\partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_1\partial x_n}\\ 
\frac{\partial^2 f}{\partial x_2\partial x_1} & \frac{\partial^2 f}{\partial x_2^2} & \cdots & \frac{\partial^2 f}{\partial x_2\partial x_n} \\ 
\cdots & \cdots & \cdots & \cdots\\ 
\frac{\partial^2 f}{\partial x_n\partial x_1} & \frac{\partial^2 f}{\partial x_n\partial x_2} & \cdots & \frac{\partial^2 f}{\partial x_n^2}
\end{bmatrix}
$$

- 它的所有元素是二阶偏导数，Hessian 矩阵是对称矩阵。

- Hessian 矩阵和函数凹凸性有密切关系。Hessian 矩阵正定，函数为凸函数，负定则为凹函数。

## -  极值判别法则

- 一元函数：$f(x)$ 一阶导数等于0处有极值，当 $f(x)$ 的二阶导数大于0时是极小值，当二阶导数小于0时是极大值，参考 $x^2$。

- 多元函数的极值判别法则：看 Hessian 矩阵在 $f(\boldsymbol{x})$ 的二阶导数等于0处，即驻点处。

  - 若 Hessian 矩阵是正定，函数在该点有极小值；
  - 若 Hessian 矩阵是负定，函数在该点有极大值；
  - 若 Hessian 矩阵不定，则还需要看更高阶导数。

- 矩阵正定：对于任意向量 $\boldsymbol{x}\ne \vec{0}$ ，都有 $\boldsymbol{x}^T\boldsymbol{A}\boldsymbol{x}>0$，则是正定矩阵，如果是 $\ge$，则是半正定矩阵。

- 判断原则：

  - 矩阵特征值全部大于0；

  - 矩阵所有的顺序主子式都大于0；

  - 矩阵合同于单位阵。

# 5. 线性代数高级

## - 二次型

- 纯二次项构成的函数，把含有 $n$ 个变量的二次齐次函数称为二次型：

$$
f\left ( x_1,x_2,\cdots,x_n \right )=a_{11}x_1^2+a_{22}x_2^2+\cdots+a_{nn}x_n^2+2a_{12}x_1x_2+2a_{13}x_1x_3+\cdots+2a_{n-1,n}x_{n-1}x_{n}
$$

- 它其实是向量和矩阵相乘的结果：$\boldsymbol{x}^T\boldsymbol{A}\boldsymbol{x}=\sum\limits_{i=1}^{n}\sum\limits_{j=1}^{n}a_{ij}x_ix_j$，$\boldsymbol{A}$ 即二次型矩阵。

$$
\left ( x_1,\cdots,x_n \right )\begin{bmatrix}
a_{11} & a_{12} & \cdots & a_{1n}\\ 
a_{21} & a_{22} & \cdots & a_{2n}\\ 
\cdots & \cdots & \cdots & \cdots\\ 
a_{n1} & a_{n2} & \cdots & a_{nn}
\end{bmatrix}\begin{pmatrix}
x_1\\ 
\cdots\\ 
\cdots\\ 
x_n
\end{pmatrix}
$$

- 机器学习中常见形式，比如是一次型：$f(\boldsymbol{x};\boldsymbol{w})=\boldsymbol{w}^T\boldsymbol{x}+b$，或者二次型：$f(\boldsymbol{x};\boldsymbol{w})=\boldsymbol{x}^T\boldsymbol{w}\boldsymbol{x}+b$。
- 回看 Hessian 矩阵：对于二次型函数，$f(\boldsymbol{x})=\boldsymbol{x}^T\boldsymbol{A}\boldsymbol{x}$：
  - $f(\boldsymbol{x})>0,x\ne0,x\in \mathbb{R}$，则 $f$ 为正定二次型，$A$ 为正定矩阵；
  - $f(\boldsymbol{x})\ge0,x\ne0,x\in \mathbb{R}$，则 $f$ 为半正定二次型，$A$ 为半正定矩阵；
  - $f(\boldsymbol{x})<0,x\ne0,x\in \mathbb{R}$，则 $f$ 为负定二次型，$A$ 为负定矩阵;
  - $f(\boldsymbol{x})\le0,x\ne0,x\in \mathbb{R}$，则 $f$ 为半负定二次型，$A$ 为半负定矩阵;
  - 以上皆不是，不定。

## - 特征值和特征向量

- 矩阵与向量的乘法相当于对向量做了一个线性变换，变换后不一定和原来在一条直线上。

- 设 $\boldsymbol{A}$ 是 $n$ 阶方阵，若存在数 $\lambda$ 和非零 $n$ 维向量 $\boldsymbol{x}$，使得 $\boldsymbol{A}\boldsymbol{x}=\lambda \boldsymbol{x}$ 成立，则称 $\lambda$ 是矩阵 $\boldsymbol{A}$ 的一个特征值 (characteristic value) 或本征值 (eigenvalue)。

- $\boldsymbol{Ax}=\lambda \boldsymbol{x} \Rightarrow \left ( \boldsymbol{A}-\lambda \boldsymbol{I }\right )\boldsymbol{x}=0$，有非零解的充要条件是系数行列式: 

  $$\left | \lambda \boldsymbol{I}-\boldsymbol{A} \right |=0$$

- $$\left | \lambda \boldsymbol{I}-\boldsymbol{A} \right |=\lambda ^n+\alpha _1\lambda ^{n-1}+\alpha _2\lambda ^{n-2}+\cdots+\alpha _{n-1}\lambda+\alpha _n$$

- 5 次和 5 次以上代数方程没有求根公式，工程上计算矩阵特征值使用 QR 算法。

- $$tr(\boldsymbol{A}) = \sum\limits_{i=1}^{n}a_{ii}=\lambda _1+\lambda _2+\cdots+\lambda _n=\sum\limits_{i=1}^{n}\lambda _i$，$\prod\limits_{i=1}^{n}\lambda _i=\left | \boldsymbol{A} \right |$$

- `np.linalg.eig(X)`

## - 特征值分解

- $n\times n$ 矩阵 $\boldsymbol{A}$ 的 $n$ 个特征值 $\lambda_1\le\lambda_2\le\cdots\le\lambda_n$，以及这 $n$ 个特征值所对应的特征向量 $\begin{pmatrix}w_1 & w_2 & \cdots & w_n \end{pmatrix}$，那么矩阵 $\boldsymbol{A}$ 可以用特征分解表示：$ \boldsymbol{A}=\boldsymbol{W}\boldsymbol{\Sigma}\boldsymbol{W}^{-1}$。特征向量可被正交单位化从而使 $\boldsymbol{W}$ 为正交矩阵。

- 定理1：设 $\boldsymbol{M}$ 为 $n\times n$ 的矩阵，其特征值为 $\lambda_1,\lambda_2,\cdots,\lambda_n$，特征向量为 $\boldsymbol{V}_1,\boldsymbol{V}_2,\cdots,\boldsymbol{V}_n$，形成线性无关集合，以每个特征向量为列构成矩阵 $\boldsymbol{A}= \begin{bmatrix}\boldsymbol{V}_1 & \boldsymbol{V}_2 & \cdots & \boldsymbol{V}_n \end{bmatrix}$。矩阵 $\boldsymbol{A}$ 可以将矩阵 $\boldsymbol{M}$ 对角化，乘积矩阵 $\boldsymbol{A}^{-1}\boldsymbol{M}\boldsymbol{A}$ 的主对角元素是矩阵 $\boldsymbol{M}$ 的特征值：
  $$
  \boldsymbol{A}^{-1}\boldsymbol{M}\boldsymbol{A}=\begin{pmatrix}\lambda_1 & 0 & \cdots & 0\\ 0 & \lambda_2 &\cdots & 0\\ \vdots  & \vdots  & \ddots  &\vdots  \\ 
  0 & 0 & \cdots  & \lambda_n \end{pmatrix}
  $$
  反之，若存在可逆矩阵 $\boldsymbol{A}$ ，使 $\boldsymbol{A}^{-1}\boldsymbol{M}\boldsymbol{A}$ 为对角矩阵，则矩阵 $\boldsymbol{A}$ 的列等于矩阵 $\boldsymbol{M}$ 的特征向量， $\boldsymbol{A}^{-1}\boldsymbol{M}\boldsymbol{A}$ 的主对角元素为矩阵 $\boldsymbol{M}$ 的特征值。

- 正交矩阵 $\boldsymbol{P}^{-1}=\boldsymbol{P}^T$ ，行和列相互之间是正交的。

- 特征分解 (Eigendecomposition)，又称谱分解 (Spectral decomposition)，只有可对角化矩阵才可以作特征分解。一个矩阵可以拆分成一个正交阵和对角矩阵以及正交阵的逆的乘积。

## - 多元函数的泰勒展开

$$
f(\boldsymbol{x})=f(\boldsymbol{x}_k)+[\nabla f(\boldsymbol{x}_k)]^T(\boldsymbol{x}-\boldsymbol{x}_k)+\frac {1}{2}(\boldsymbol{x}-\boldsymbol{x}_k)^TH(\boldsymbol{x}_k)(\boldsymbol{x}-\boldsymbol{x}_k)+\boldsymbol{o}^n
$$

注：$\nabla f(\boldsymbol{x}_k)$ 是梯度，$H(\boldsymbol{x}_k)$ 是 Hessian 矩阵，$\boldsymbol{x}^T\boldsymbol{A}\boldsymbol{x }\Rightarrow ax^2$ 。

## - 矩阵和向量的求导公式

- $\nabla (\boldsymbol{w}^T\boldsymbol{x})=\boldsymbol{w}$
- $\nabla (\boldsymbol{x}^T\boldsymbol{A}\boldsymbol{x})= (\boldsymbol{A}+\boldsymbol{A}^T)\boldsymbol{x}$
- $\nabla ^2 (\boldsymbol{x}^T\boldsymbol{A}\boldsymbol{x})= \boldsymbol{A}+\boldsymbol{A}^T$ 二阶导即再对 $\boldsymbol{x}$ 求导。

## - 奇异值分解 (SVD)

- 可以应用于任意形状的矩阵，区别于谱分解；
- $\boldsymbol{A}=\boldsymbol{U}\boldsymbol{\Sigma}\boldsymbol{V}^T$，其中 $\boldsymbol{A}$ 是 $m\times n$ 的矩阵， $\boldsymbol{U}$, $\boldsymbol{V}$ 都是正交矩阵，$\boldsymbol{\Sigma}$ 是对角阵 $m\times n$ ；
- $\boldsymbol{U}$ 是 $\boldsymbol{A}\boldsymbol{A}^T$ 正交化特征向量构成的 $m\times m$ 矩阵，$\boldsymbol{V}$ 是 $\boldsymbol{A}^T\boldsymbol{A}$ 正交化特征向量构成的 $n\times n$ 矩阵；
- $\boldsymbol{\Sigma}$ 是 $m\times n$ 的矩阵，除了主对角线上的元素 (奇异值) 以外全部为0， $\boldsymbol{U}$, $\boldsymbol{V}$ 都是酉矩阵，即 $\boldsymbol{U}^T\boldsymbol{U}=\boldsymbol{I}$, $\boldsymbol{V}^T\boldsymbol{V}=\boldsymbol{I}$ 

## - 求解奇异值分解

- $n\times n$ 方阵 $\boldsymbol{A}^T\boldsymbol{A}$ 求 $n$ 个特征向量：$(\boldsymbol{A}^T\boldsymbol{A})\boldsymbol{v}_i=\lambda_i\boldsymbol{v}_i$ ，将所有特征向量张成 $n\times n$ 的矩阵 $\boldsymbol{V}$ ，其中每个特征向量叫 $\boldsymbol{A}$ 的右奇异向量；

- $m\times m$ 方阵 $\boldsymbol{A}\boldsymbol{A}^T$ 求 $m$ 个特征向量：$(\boldsymbol{A}\boldsymbol{A}^T)\boldsymbol{u}_i=\lambda_i\boldsymbol{u}_i$ ，将所有特征向量张成 $m\times m$ 的矩阵 $\boldsymbol{U}$ ，其中每个特征向量叫 $\boldsymbol{A}$ 的左奇异向量；

- $$
  \boldsymbol{A}=\boldsymbol{U}\boldsymbol{\Sigma}\boldsymbol{V}^T\Rightarrow
  \boldsymbol{A}\boldsymbol{V}=\boldsymbol{U}\boldsymbol{\Sigma}\boldsymbol{V}^T\boldsymbol{V}\Rightarrow
  \boldsymbol{A}\boldsymbol{V}=\boldsymbol{U}\boldsymbol{\Sigma}\Rightarrow
  \boldsymbol{A}\boldsymbol{v}_i=\boldsymbol{\sigma}_i\boldsymbol{u}_i\Rightarrow
\boldsymbol{\sigma}_i=\boldsymbol{A}\boldsymbol{v}_i/\boldsymbol{u}_i
  $$

- $$
  \boldsymbol{A}^T\boldsymbol{A}=\boldsymbol{V}\boldsymbol{\Sigma}^2\boldsymbol{V}^T,\boldsymbol{A}\boldsymbol{A}^T=\boldsymbol{U}\boldsymbol{\Sigma}^2\boldsymbol{U}^T
  $$

  $\boldsymbol{A}^T\boldsymbol{A}$ 特征值与奇异值：$\sigma_i=\sqrt{\lambda_i}$

## - 奇异值分解的性质

- 奇异值矩阵中按照从大到小排列，且减少得特别快，即可以用最大的 $k$ 个奇异值和对应的左右奇异向量来近似描述矩阵：

  $$\boldsymbol{A}_{m\times n}=\boldsymbol{U}_{m\times m}\boldsymbol{\Sigma}_{m\times n}\boldsymbol{V}_{n\times n}^T\approx \boldsymbol{U}_{m\times k}\boldsymbol{\Sigma}_{k\times k}\boldsymbol{V}_{n\times k}^T$$

  大的矩阵用三个小矩阵近似描述；

- 若 $\boldsymbol{\Sigma}$ 中有 $k$ 个非0值：$\sigma_1\ge \sigma_2\ge\cdots\ge\sigma_k>0$，则此时乘回去即是 $\boldsymbol{A}$；

## - SVD 的应用

### - 数据压缩

```python
import numpy as np
u, sigma, v = np.linalg.svd(arr)
new_arr = np.mat(u[:,0:2])*np.mat(np.diag(sigma[0:2]))*np.mat(v[0:2,:])
np.rint(new_arr)
```

### - PCA 降维

- PCA (principal components analysis) 主成分分析

- 总体方差：$\sigma^2=\frac{\sum(X-\mu)^2}N$，样本方差：$s^2=\frac{\sum (X-\bar{X})}{n-1}$，

  $D(X)=E[(X-E(X))^2]=E(X^2)-[E(X)]^2$，

  $D(X\pm Y)=D(X)+D(Y)\pm2Cov(X,Y)$，$Cov(X,Y)=E\{[X-E(X)][Y-E(Y)]\}$

- 左奇异向量压缩行，右奇异向量压缩列，即取奇异值较大的左奇异向量或右奇异向量与原数据相乘。

### - 协调过滤

- 用户推荐
- 用 SVD 分解把样本映射到低维空间

### - 矩阵求逆

- 奇异值求倒数：$\boldsymbol{A}=\boldsymbol{U}\boldsymbol{\Sigma}\boldsymbol{V}^T\Rightarrow \boldsymbol{A}^{-1}=\boldsymbol{V}\boldsymbol{\Sigma}^{-1}\boldsymbol{U}^T$

# 6. 概率论

## - 基本概念

- 用概率论建模，假设它服从某种概率分布

- 随机事件 (必然事件，不可能事件) 和随机事件概率

- 随机事件独立：

  $$p(b|a)=p(b)$$ , $p(a,b)=p(a)p(b)$, $p(a_1,\cdots,a_n)=\prod\limits_{i=1}^{n}p(a_i)$

- 随机变量：(1) 离散；(2) 连续，概率密度函数 $f(x)\ge0,\int_{-\infty}^{+\infty}f(x)dx=1$

  $F(y)=p(x\le y)=\int_{-\infty}^{y}f(x)dx,\int_{x_1}^{x_2}f(x)dx=F(x_2)-F(x_1)$ 


## - 条件概率和贝叶斯公式

- 对于两个随机事件 $a$ 和 $b$ ，在 $a$ 发生的情况下 $b$ 发生的概率为 $$p(b|a)=\frac{p(a,b)}{p(a)}$$

- 贝叶斯公式：$$p(a|b)=\frac{p(a)p(b|a)}{p(b)}$$，$a$ 为因，$b$ 是果，知道原因后结果发生的概率是先验概率，贝叶斯公式得到的是后验概率。

  $$p(b)p(a|b)=p(a,b)=p(a)p(b|a)$$

- 最大化后验概率 MAP

## - 数学期望和方差

- 数学期望就是概率意义的平均值

  $E(x)=\sum x_ip(x_i),E(x)=\int_{-\infty}^{+\infty}xf(x)dx$

- 方差反应数据的波动程度

  $D(x)=\sum (x_i-E(x))^2p(x_i)$

  $D(x)=\int_{-\infty}^{+\infty} (x-E(x))^2f(x)dx$

- 有关性质

  $E(a+bX)=a+bEX$

  $VarX=E(X-\mu)^2=E(X^2)-(E(X))^2$

## - 常用分布

- 正态分布：$\sigma$ 越大越矮胖
  $$
  f(x)=\frac{1}{\sqrt{2\pi }\sigma }e^{-\frac{(x-\mu )^2}{2\sigma ^2}}
  $$

- 均匀分布：
  $$
  f(x)=\left\{\begin{matrix}
  \frac{1}{b-a}\quad a\le x\le b\\ 
  0\quad x<a,x>b
  \end{matrix}\right.
  $$

- 二项分布
  $$
  p(x=1)=p,p(x=0)=1-p
  $$

## - 随机向量

- 随机变量推广至随机向量

- 离散型：$p(\boldsymbol{x}=\boldsymbol{x}_i)$ 向量取值为某一向量

- 连续型：$f(\boldsymbol{x})\ge 0, \iiint f(\boldsymbol{x})d\boldsymbol{x}=1$，二维时：$f(x_1,x_2)\ge 0, f(x)\ge0,\int_{-\infty}^{+\infty}f(x_1,x_2)dx_1dx_2=1$

- 随机变量的独立性：$f(x_1,x_2,\cdots,x_n)=f(x_1)f(x_2)\cdots f(x_n)$

- 随机向量的常见分布：正态分布
  $$
  f(x)=\frac{1}{(2\pi)^{n\over 2}\left | \Sigma  \right |^{1\over 2}}e^{-\frac{1}{2}(x-\mu)^T\Sigma ^{-1}(x-\mu)}\ ,x\in \mathbb{R}^n
  $$
  $\Sigma$ 是协方差矩阵。

## - 协方差

- 对于两个随机变量：
  $$
  cov(x_1,x_2)=E((x_1-E(x_1))(x_2-E(x_2)))\\
  cov(x_1,x_2)=E(x_1x_2)-E(x_1)E(x_2)
  $$

- 协方差矩阵
  $$
  \begin{bmatrix}
  x_1x_1 & x_1x_2 & \cdots  & x_1x_n\\ 
  x_2x_1 & \ddots  & \cdots  & x_2x_n\\ 
  \vdots  & \vdots  & \ddots  & \vdots \\ 
  x_nx_1 & \cdots  & \cdots  & x_nx_n
  \end{bmatrix}
  $$

## - 最大似然估计 

- 最大似然估计(maximum likelihood estimation, MLE)，估计参数；
- 总似然：$L(\theta )= \prod\limits_{i=1}^{l}p(x_i;\theta )$
- 对数总似然：$\ln L(\theta )= \ln\prod\limits_{i=1}^{l}p(x_i;\theta )=\sum\limits_{i=1}^{l}\ln p(x_i;\theta )$
- $\max \sum\limits_{i=1}^{l}\ln p(x_i;\theta )$，对 $\theta$ 求导让它对于0

# 7. 最优化

##  - 基本概念

- 求 $f(x)$ 的极大值或极小值，$x$ 是优化变量，就是自变量，$f(x)$ 是目标函数，可能带有约束条件，满足约束并在定义域内的集合叫可行域；
  $$
  \max f(x) \Leftrightarrow\min f(x)\\
  g_i(x)=0,\quad i=1,\cdots,m\\
  h_j(x)\le 0\quad j=1,\cdots,n
  $$

- 局部极小值：任意在 $x_0$ 的领域存在，$f(x)\ge f(x_0), \forall x\in \delta (x_0)$

- 通过大量实践发现在高维度的优化问题中，局部极小值 (local minimum)和全局极小值没有太大的区别，甚至有时候有更好的泛化能力。

- 为什么要迭代求解？(求导困难，求根困难)，(初始值，逼近)

## - 梯度下降法

$$
\boldsymbol{x}_{k+1}=\boldsymbol{x}_k-\gamma \nabla f(\boldsymbol{x}_k)
$$

推导：

1. 利用多元函数的泰勒展开公式：$f(\boldsymbol{x})-f(\boldsymbol{x}_0)\approx[\nabla f(\boldsymbol{x}_0)]^T(\boldsymbol{x}-\boldsymbol{x}_0)$
2. $$X^TY=|X|\cdot|Y|\cdot\cos\theta$$，$\cos\theta=-1$ 下降幅度最大
3. 为了使得下降幅度最大，向量 $\boldsymbol{x}-\boldsymbol{x}_0$ (不一定是单位向量) 的方向和梯度方向相反：$\boldsymbol{v}=-\frac{\nabla f(\boldsymbol{x}_0)}{\left \| \nabla f(\boldsymbol{x}_0) \right \|}$
4. $\boldsymbol{x}=\boldsymbol{x}_0-\eta\frac{\nabla f(\boldsymbol{x}_0)}{\left \| \nabla f(\boldsymbol{x}_0) \right \|}$，分母是标量可并入 $\eta$，即 $\boldsymbol{x}=\boldsymbol{x}_0-\eta\nabla f(\boldsymbol{x}_0)$

$\eta$ 是步长，不能太大，否则不满足约等于条件。

## - 牛顿法

$$
\boldsymbol{x}_{k+1}=\boldsymbol{x}-\boldsymbol{H}_k^{-1}\boldsymbol{g}_k
$$

思想：找梯度为0的点。

推导：

1. 多元函数的泰勒展开公式展开二次以上的项
   $$
   f(\boldsymbol{x})=f(\boldsymbol{x}_0)+[\nabla f(\boldsymbol{x}_0)]^T(\boldsymbol{x}-\boldsymbol{x}_0)+\frac {1}{2}(\boldsymbol{x}-\boldsymbol{x}_0)^TH(\boldsymbol{x}_0)(\boldsymbol{x}-\boldsymbol{x}_0)+\boldsymbol{o}(\boldsymbol{x}-\boldsymbol{x}_0)
   $$
   取近似
   $$
   f(\boldsymbol{x})\approx f(\boldsymbol{x}_0)+[\nabla f(\boldsymbol{x}_0)]^T(\boldsymbol{x}-\boldsymbol{x}_0)+\frac {1}{2}(\boldsymbol{x}-\boldsymbol{x}_0)^TH(\boldsymbol{x}_0)(\boldsymbol{x}-\boldsymbol{x}_0)
   $$

2. 由于 $(\boldsymbol{w}^T\boldsymbol{x})'=\boldsymbol{w}$，$(\boldsymbol{x}^T\boldsymbol{A}\boldsymbol{x})'= (\boldsymbol{A}+\boldsymbol{A}^T)\boldsymbol{x}$，故有：
   $$
   \nabla f(\boldsymbol{x})\approx \nabla f(\boldsymbol{x}_0)+H(\boldsymbol{x}_0)(\boldsymbol{x}-\boldsymbol{x}_0)=\boldsymbol{g}+\boldsymbol{H}(\boldsymbol{x}-\boldsymbol{x}_0)
   $$

3. 令 $\nabla f(\boldsymbol{x})=0$，如果 Hessian 矩阵可逆，则有
   $$
   \boldsymbol{g}+\boldsymbol{H}(\boldsymbol{x}-\boldsymbol{x}_0)=0\\
   \Rightarrow \boldsymbol{x}-\boldsymbol{x}_0=-\boldsymbol{H}^{-1}\boldsymbol{g}
   $$

对比：
$$
\boldsymbol{x}_{k+1}=\boldsymbol{x}_k-\eta\cdot\boldsymbol{g}_k\\
\boldsymbol{x}_{k+1}=\boldsymbol{x}_k-\eta\cdot\boldsymbol{H}^{-1}_k\cdot\boldsymbol{g}_k
$$

- 牛顿法步长设定不好就有可能不收敛，不是迭代就一定使得函数值下降，一般用 line search 的技术，选择一些值如 $10^{-4},10^{-6}$，看哪个步长使得 $f(\boldsymbol{x}_{k+1})$ 更小。

- 牛顿法收敛更快。

## - 坐标下降法

- 分治 (分而治之) 法的思想：保持其他不动，只优化其中一个，优化完了之后再回来重新优化。
- 计算量小

## - 数值优化算法面临的问题

- 驻点不一定是极值点
- 局部极值问题；
- 鞍点问题，如 $x^3$，在这一点 Hessian 矩阵不定，

## - 凸优化问题

前面数值优化面临两个问题，对这类问题进行限定：

1. 优化变量的可行域必须是凸集；
2. 优化函数必须是个凸函数。

同时满足这两个条件的叫凸优化问题，才能说局部极小值就是全局极小值。

### - 凸集

- 定义：对于一个点的集合 $C$，有属于它的两个点 $x,y$，它们两点连线中任意一点也属于该集合：$\theta x+(1-\theta)y\in C,0\le\theta\le1$
- 典型的凸集：
  - 欧式空间 $\mathbb{R}^n$：$\boldsymbol{x},\boldsymbol{y} \in \mathbb{R}^n\Rightarrow \theta \boldsymbol{x} +(1-\theta)\boldsymbol{y}\in \mathbb{R}^n$；很多可行域就是欧式空间，即凸集；
  - 仿射子空间：$\left \{ \boldsymbol{x}\in\mathbb{R}^n:\boldsymbol{A}\boldsymbol{x}=\boldsymbol{b} \right \}$，$\boldsymbol{x}$ 是 $n$ 维欧式空间的向量，满足线性方程的解；所有等式约束构成的集合是凸集，不会构建非线性等式约束；
  - 多面体：$\left \{ \boldsymbol{x}\in\mathbb{R}^n:\boldsymbol{A}\boldsymbol{x}\le\boldsymbol{b} \right \}$，线性不等式的解；一组线性不等式约束，也是凸集。
- 凸集的交集也是凸集 $\bigcap\limits_{i=1}^{k}C_i$，并集不一定是凸集。

### - 凸函数

- 定义：函数上任意两点它们的连线 (即割线) 上的值比对应的函数上的值要大，$f(\theta x+(1-\theta )y)<\theta f(x)+(1-\theta )f(y)$
- 凸函数的证明：
  1. 利用定义
  2. 利用一阶导数：
     - 一元函数：$f(y)\ge f(x)+f'(x)(y-x)$
     - 多元函数：$f(\boldsymbol{y})\ge f(\boldsymbol{x})+\nabla f(\boldsymbol{x})^T(\boldsymbol{y}-\boldsymbol{x})$
  3. 二阶判别法：
     - 一元函数：$f''(x)\ge 0$
     - 多元函数：Hessian 矩阵半正定，$>0$ 是严格凸函数
- 如果每个函数 $f_i(x)$ 都是凸函数，那么它们的非负线性组合 $f(x)=\sum\limits_{i=1}^{k}w_if_i(x),w_i\ge 0$ 也是凸函数。

### - 凸优化的性质

目标函数是凸函数，可行域是凸集，则局部最优解一定是全局最优解。

证明：(反证法)

假设有一点 $x$ 是局部最小值，但不是全局最小值，则存在另一个点 $y$ 是全局最小值，这时 $f(y)<f(x)$。

证明 $x$ 的领域有一个点 $z$ 比 $x$ 小即可，取 $z=\theta y+(1-\theta)x,\theta=\frac{\delta}{2\|x-y\|_2}$ 即可。

### - 凸优化一般的表述形式

$$
\min f(x),x\in C
$$

或者
$$
\min f(x)\\
c_i(x)\le0,i=1,\cdots,m\\
h_j(x)=0,j=1,\cdots,k
$$

## - 拉格朗日乘数法

将一个有n 个变量与k 个约束条件的最优化问题转换为一个有n + k个变量的方程组的极值问题，其变量不受任何约束；

(1) 等式约束条件
$$
\min f(\boldsymbol{x})\\
s.t.\quad h_k(\boldsymbol{x})=0 \quad k=1,2,\cdots,l
$$
求解步骤：

1. 定义拉格朗日函数：
   $$
   F(\boldsymbol{x},\boldsymbol{\lambda})=f(\boldsymbol{x})+\sum\limits_{k=1}^l\lambda_kh_k(\boldsymbol{x})
   $$

2. 解变量的偏导方程：
   $$
   \frac{\partial F}{\partial x_i}=0,\cdots,\frac{\partial F}{\partial \lambda _k}=0,\cdots
   $$

3. 或者说是分别对 $\boldsymbol{x}$ 和 $\boldsymbol{\lambda}$ 求梯度，然后解方程组
   $$
   \nabla_xf+\sum\limits_{k=1}^l\lambda_k\nabla_xh_k=0\\
   h_k(\boldsymbol{x})=0
   $$

(2) 带不等式约束条件

可参考KKT条件。

## - 拉格朗日对偶

$$
\min f(x)\\
g_i(x)\le0,i=1,\cdots,m\\
h_j(x)=0,j=1,\cdots,k
$$

构建一个广义 (包括不等式约束) 的拉格朗日函数：
$$
L(x,\alpha,\beta)=f(x)+\sum\limits_{i=1}^m\alpha_ig_i(x)+\sum\limits_{j=1}^k\beta_ih_j(x),\alpha_i\ge 0
$$
问题转化为：
$$
p^*=\min_x \max_{\alpha,\beta,\alpha_i\ge 0}L(x,\alpha,\beta)=\min_x\theta_p(x)
$$
理解可参考：[【数学】拉格朗日对偶，从0到完全理解](https://blog.csdn.net/frostime/article/details/90291392)

无论如何，$p^*$ 都不会小于 $\max\limits_{\alpha,\beta,\alpha_i\ge 0}L(x,\alpha,\beta)$。

## - KKT 条件

$$
\min f(x)\\
g_i(x)\le0,i=1,\cdots,q\\
h_j(x)=0,j=1,\cdots,p
$$

$$
L(x,\lambda,\mu)=f(x)+\sum\limits_{j=1}^p\lambda_jh_j(x)+\sum\limits_{i=1}^q\mu_ig_i(x)
$$

KKT 条件：
$$
\nabla_x L(x^*)=\nabla f(x^*) +\sum\limits_{j=1}^p\lambda_i^*\nabla h_j(x^*)+\sum\limits_{i=1}^q\mu_i^*\nabla g_i(x^*)=0\\
\mu_i^*\ge0\\
\mu_i^*g_i(x^*)=0\\
h_j(x^*)=0\\
g_i(x^*)\le0
$$
